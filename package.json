{
  "name": "llm-evaluation-suite",
  "version": "1.0.0",
  "description": "LLM evaluation suite using PromptFoo and Claude-as-Judge",
  "type": "module",
  "main": "index.js",
  "directories": {
    "test": "tests"
  },
  "scripts": {
    "eval": "promptfoo eval",
    "eval:watch": "promptfoo eval --watch",
    "view": "promptfoo view",
    "judge": "node judges/claude-judge.js",
    "test:example": "promptfoo eval -c config/example-config.yaml",
    "install-hooks": "bash scripts/install-hooks.sh",
    "check-results": "node scripts/check-results.js",
    "enforce-quality": "node scripts/enforce-quality.js",
    "estimate-cost": "node scripts/estimate-cost.js",
    "type-check": "echo 'JavaScript project - no type checking'",
    "lint": "echo 'Linting not yet configured - see CLAUDE.md'",
    "test": "npm run test:example"
  },
  "keywords": ["llm", "evaluation", "testing", "promptfoo", "claude"],
  "author": "Kalpesh",
  "license": "ISC",
  "dependencies": {
    "@anthropic-ai/sdk": "^0.65.0",
    "dotenv": "^17.2.3",
    "promptfoo": "^0.118.12"
  }
}
